<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Consume Reality | Ali Garabaglu </title> <meta name="author" content="Ali Garabaglu"> <meta name="description" content="How humans observe reality and how machines can learn from it"> <meta name="keywords" content="Ali Garabaglu, physics, physicist, particle physics, academic research, machine learning, ML, AI"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Ali Garabaglu"
        },
        "url": "https://alig146.github.io/blog/2025/obervation/",
        "@type": "BlogPosting",
        "description": "How humans observe reality and how machines can learn from it",
        "headline": "How to Consume Reality",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Ali Garabaglu",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="" integrity="" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href=""> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%8A&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alig146.github.io/blog/2025/obervation/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ali Garabaglu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="https://alig146.github.io/assets/pdf/garabaglu_ali_cv.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How to Consume Reality</h1> <p class="post-meta"> Created on August 03, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/physics"> <i class="fa-solid fa-hashtag fa-sm"></i> physics</a>   <a href="/blog/tag/research"> <i class="fa-solid fa-hashtag fa-sm"></i> research</a>   <a href="/blog/tag/philosophy-of-science"> <i class="fa-solid fa-hashtag fa-sm"></i> philosophy-of-science</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   ·   <a href="/blog/category/physics"> <i class="fa-solid fa-tag fa-sm"></i> physics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In both science and everyday experience, understanding the world begins with observation. We observe phenomena, analyze them, and construct models that help us explain, predict, and manipulate what we perceive. This process is central not only to human understanding but also increasingly to artificial intelligence , which now plays an active role in how we model reality.</p> <p>But what if we stepped back and re-examined the entire process of observation—not just as a passive act of seeing, but as an active, layered mechanism? And what if the traditional human-centered model of observation introduces limitations that AI could overcome?</p> <p>Let’s explore this idea by dissecting the observation pipeline into three fundamental stages: <strong>Reality → Observation → Model</strong>. Each stage plays a distinct role, and understanding their relationship can help us design AI systems that don’t just mimic human perception—but perhaps transcend it.</p> <p>Reality is the only stage that exists independently. It is the raw, unprocessed fabric of the universe, existing whether or not there is someone—or something—there to observe it. Rocks fall, stars explode, and particles interact, regardless of our awareness.</p> <p>This concept aligns with the scientific worldview: reality is objective and exists outside the observer. However, some interpretations in physics, such as the Anthropic Principle, suggest that our very presence as observers influences the kind of universe we can perceive or exist in. While intriguing, this philosophical twist doesn’t negate reality’s independence—it merely highlights that what we observe of it is constrained by our ability to exist within it.</p> <p>Observation sits between reality and the model. It is the middleman. It is the act of filtering raw reality through the senses, brain, and prior beliefs of the observer. This step is powerful, but it’s also perilous—because it is deeply subjective.</p> <p>When humans observe, we don’t take in reality in its raw form. Instead, we compress it. Our eyes detect only a narrow slice of the electromagnetic spectrum; our ears hear within a specific frequency range. Our brains process these signals through a complex web of prior experiences, language, and cultural context. What we “see” is never pure reality—it’s a filtered, interpreted, and often biased approximation.</p> <p>In a sense, the human brain acts like a compression algorithm, reducing the overwhelming detail of reality into a manageable model. This makes survival possible, but it also limits our understanding.</p> <p>Once observation has occurred, the mind builds a model, an internal representation of the external world. These models help us reason, predict, and make decisions. They’re not exact replicas of reality, but simplified versions tuned for efficiency, not accuracy.</p> <p>Models are everywhere. Newton’s laws model motion. Economic models predict markets. Even a child’s understanding that “fire is hot” is a model formed through limited experience.</p> <p>Artificial Intelligence, too, builds models. An AI trained to recognize cats doesn’t understand what a cat is, it compresses vast data into a pattern-recognition system that predicts “cat” when it sees certain features.</p> <p>In the traditional AI pipeline, humans serve as the “observer.” We decide what data to collect, what labels to assign, and how to preprocess it. We effectively insert ourselves between reality and the AI’s model, bringing all our human biases along for the ride.</p> <p>This raises a profound question: What if we could remove the human observer from the loop?</p> <p>Instead of feeding AI a human-preprocessed version of reality, what if we gave it raw interactions, measurements and allowed it to form its own model, untethered from our interpretations? We could perhaps allow the machine to take in all frequencies of light, acoustics and any other sensor we have at hand.</p> <p>Doing so would allow AI to develop intra-realities, internal representations of the world that aren’t inherited from humans, but emerge from the AI’s direct interaction with raw data. These intra-realities could differ from ours in fascinating, potentially more efficient ways.</p> <p>In some domains, like reinforcement learning, AI systems already do this to an extent—interacting with an environment and learning from the consequences. But even here, the environment is usually shaped by human design. The challenge—and opportunity—is to let AI connect more directly to reality, minimizing the biases of the middle step: human observation.</p> <p>The traditional chain—<strong>Reality → Observation → Model</strong>—has served humanity well. It is the foundation of science, perception, and learning. But as we build artificial systems capable of modeling the world, we are no longer confined to this human-centric path. AI presents an opportunity to explore entirely new ways of perceiving and understanding reality—perhaps ones that bypass the human observer altogether.</p> <p>This possibility is exciting, but also deeply challenging. Humans have spent centuries observing the world and constructing languages to describe it. These languages—mathematical, symbolic, spoken—have shaped our models of reality and, in turn, shaped what we pass on to AI systems. Today’s large language models, for example, don’t observe the world directly. They observe human descriptions of the world—textual artifacts shaped by our cognition, our biases, and our history.</p> <p>For AI to go beyond this, to observe raw reality and construct its own model, perhaps even its own language, would mark a profound shift. Such a system wouldn’t be “thinking like a human,” but rather developing a fundamentally different mode of understanding.</p> <p>Could this help machines reach the state of a Von Neumann universal constructor? Perhaps by learning raw reality we can bridge the gap between the world of bits to the world of atoms for these thinking machines. This move toward embodied intelligence aligns with the trajectory of next-generation AI: systems that don’t just read about reality but live within it, explore it, and interact with it. Language models may help AIs talk about the world. But sensorimotor models will help them exist in it.</p> <p>Yet this raises an important question: Is that what we want? If the purpose of AI is to assist humans, collaborate with us, and serve human goals, it must remain intelligible. Even if an AI could form its own reality-model from first principles, would it be able to communicate that back to us? Would we be able to understand an alien language built from patterns we never noticed or processed?</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/stagnation-of-physics/">The Great Stagnation Era of Physics</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ali Garabaglu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>